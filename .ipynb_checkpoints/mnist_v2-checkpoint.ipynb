{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a52800",
   "metadata": {},
   "source": [
    "# MNIST v2\n",
    "\n",
    "Welcome to the programming exercise portion of Topic 5! We are going to continue using the same dataset as well as basic architecture of the Neural Network in topic 4, however applying the new optimization techniques learned in topic 5. Note that the first portion of the notebook is already written for you, as it is identical to topic 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf6eab",
   "metadata": {},
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc6325c",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d30c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.read_csv(\"datasets/train.csv\")\n",
    "testset = pd.read_csv(\"datasets/test.csv\")\n",
    "\n",
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f50378",
   "metadata": {},
   "source": [
    "### Preparing the Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1addc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets all the rows, and all columns AFTER the first one\n",
    "features = trainset.iloc[:, 1:].to_numpy()\n",
    "\n",
    "print(f\"features shape: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7512813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(features[8].reshape(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519096c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates one-hot encoding out of the label-encoded classes\n",
    "labels_dummy = pd.get_dummies(trainset['label'])\n",
    "labels_dummy\n",
    "\n",
    "labels = labels_dummy.to_numpy()\n",
    "print(f\"labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda1fb0",
   "metadata": {},
   "source": [
    "### Preparing the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d24a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd235f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = testset.to_numpy()\n",
    "plt.imshow(features_test[2].reshape(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e771afe",
   "metadata": {},
   "source": [
    "### Checking for Imbalanced Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of items in each class\n",
    "print(np.sum(labels, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc185c",
   "metadata": {},
   "source": [
    "### Train-CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4583a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/CV split using the training data.\n",
    "# The testing data contains no labels, thus we cannot create CV set from it\n",
    "X_train, X_cv, Y_train, Y_cv = train_test_split(features, labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b3b2d",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "\n",
    "This is where we are going to implement a data generator to break up our dataset into mini-batches. While the mnist-digits dataset is technically small enough to fit into memory, we will build a data generator anyways for your practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454adc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(X, Y, batch_size=32):\n",
    "    pass\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5150654",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e239ae2",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "This Neural Network will use a **3-layer** design with **900, 900,** and **10** nodes in each layer. Since the input data are images, we need to flatten the **28 x 28** images into a vector of size **784**. This network is identical to the one from topic 4; we will implement the techniques learned in `Topic 5 -- Advanced Optimization` into this model.\n",
    "\n",
    "### Defining Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b953cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import Module, Linear, Softmax, BatchNorm1d, ReLU, Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.nn.init import xavier_normal, kaiming_normal\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "class NNv2(Module):\n",
    "    def __init__(self, input_dim, output_dim, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.bn_in = BatchNorm1d(input_dim)\n",
    "        self.z1 = Linear(input_dim, 900)\n",
    "        kaiming_normal(self.z1.weight)\n",
    "        self.bn1 = BatchNorm1d(900)\n",
    "        self.a1 = ReLU()\n",
    "        self.d1 = Dropout(drop)\n",
    "        \n",
    "        self.z2 = Linear(900, 900)\n",
    "        kaiming_normal(self.z2.weight)\n",
    "        self.bn2 = BatchNorm1d(900)\n",
    "        self.a2 = ReLU()     \n",
    "        self.d2 = Dropout(drop)\n",
    "        \n",
    "        self.z3 = Linear(900, output_dim)\n",
    "        xavier_normal(self.z3.weight)\n",
    "        self.bn3 = BatchNorm1d(output_dim)\n",
    "        self.a3 = Softmax(dim=1)  \n",
    "        \n",
    "        self.acc = Accuracy()\n",
    "        \n",
    "        self.val_loss = None\n",
    "        self.val_acc = None\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn_in(x)\n",
    "        \n",
    "        x = self.z1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = self.z2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = self.z3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.a3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def fit(self, t_gen, loss_fn, opt, cv_gen=None, epochs=1, train_steps=1, val_steps=1):\n",
    "        \n",
    "        writer_train = SummaryWriter(\"runs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"-train\")\n",
    "        writer_val = SummaryWriter(\"runs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"-val\")\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "            \n",
    "            # Tensorboard Writer\n",
    "#             writer_train.add_scalar(\"loss\", self.loss, i)\n",
    "#             writer_train.add_scalar(\"accuracy\", self.accuracy, i)\n",
    "#             writer_val.add_scalar(\"loss\", self.val_loss, i)\n",
    "#             writer_val.add_scalar(\"accuracy\", self.val_acc, i)\n",
    "                \n",
    "            \n",
    "      \n",
    "    \n",
    "# Custom CCE Loss function because torch doesn't have one that is suitable\n",
    "def CCE(Y_pred, Y_true):\n",
    "    Y_pred = 0.9999999*Y_pred + (1-0.9999999)/2\n",
    "    ylogy = -Y_true * torch.log(Y_pred)\n",
    "    sum_across = torch.sum(ylogy, dim=1)\n",
    "    sum_down = torch.sum(sum_across, dim=0)/Y_true.shape[0]\n",
    "    return sum_down\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ac15c",
   "metadata": {},
   "source": [
    "### Test Train\n",
    "\n",
    "Before the Hyperparameter Search process, let's test out training our model just to make sure that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32828ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c730c0",
   "metadata": {},
   "source": [
    "## Hyperparameter Search\n",
    "\n",
    "The final task in this notebook is to search for hyperparameters. Here we are going to create a random search algorithm to train multiple models and evaluate each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041103e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "input_dim, output_dim = (X_train.shape[1], 10)\n",
    "\n",
    "# Feel free to change the EPOCHS and NUM_MODELS\n",
    "EPOCHS = 100\n",
    "NUM_MODELS = 1\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "### Hyperparameters ###\n",
    "LEARNING_RATE = np.random.uniform(2e-3, 5e-3, NUM_MODELS)\n",
    "L2 = np.random.uniform(5e-5, 9e-5, NUM_MODELS)\n",
    "DROP = np.random.uniform(0.4, 0.6, NUM_MODELS)#0.4, 0.6\n",
    "#######################\n",
    "\n",
    "\n",
    "\n",
    "T_STEPS = int(X_train.shape[0]/BATCH_SIZE)\n",
    "V_STEPS = int(X_cv.shape[0]/BATCH_SIZE)\n",
    "t_gen = gen(X_train, Y_train, BATCH_SIZE)\n",
    "v_gen = gen(X_cv, Y_cv, BATCH_SIZE)\n",
    "\n",
    "hparams = pd.DataFrame(columns=['val_acc', 'val_loss', 'L2', 'Learning Rate', 'Dropout'])\n",
    "model=None\n",
    "\n",
    "for i in range(NUM_MODELS):\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f\"##### MODEL {i+1}/{NUM_MODELS} #####\")\n",
    "    model = NNv2(input_dim, output_dim, drop=DROP[i]).cuda()\n",
    "    optimizer = Adam(model.parameters(), weight_decay=L2[i], lr=LEARNING_RATE[i])\n",
    "    criterion = CCE\n",
    "\n",
    "    model.fit(t_gen, criterion, optimizer, epochs=EPOCHS, train_steps=T_STEPS, cv_gen=v_gen, val_steps=V_STEPS)\n",
    "    \n",
    "    \n",
    "    history = pd.DataFrame([[model.val_acc.cpu().item(), model.val_loss.cpu().item(), L2[i], LEARNING_RATE[i], DROP[i]]], \n",
    "                          columns=['val_acc', 'val_loss', 'L2', 'Learning Rate', 'Dropout'])\n",
    "    hparams = hparams.append(history, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    print('\\n\\n')\n",
    "    \n",
    "#     del model\n",
    "#     del optimizer\n",
    "#     del criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = hparams.sort_values(by=['val_acc'], ascending=False)\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f25cf5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf87f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.cpu().eval()\n",
    "\n",
    "# make predictions on the first 20 examples of X_test_t\n",
    "Y_test_t = model(torch.from_numpy(features_test[:20, :]).float())\n",
    "\n",
    "# Convert the first 20 examples from torch.tensor\n",
    "# to np.ndarray so we can plot it\n",
    "X_test = features_test[:20, :]\n",
    "Y_test = Y_test_t.cpu().detach().numpy()\n",
    "\n",
    "# Create subplots with 20 subplots\n",
    "fig, axes = plt.subplots(X_test.shape[0], 1, figsize=(100, 100))\n",
    "\n",
    "# each element in axes will contain an image plot\n",
    "for i in range(X_test.shape[0]):\n",
    "    axes[i].imshow(X_test[i].reshape((28, 28)))\n",
    "    axes[i].title.set_text(f'My Prediction: {np.argmax(Y_test, axis=1)[i]}')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0eee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscript = torch.jit.trace(model.cpu(), torch.rand((1, 784)))\n",
    "tscript.save('mnist_predictor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14e12a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
