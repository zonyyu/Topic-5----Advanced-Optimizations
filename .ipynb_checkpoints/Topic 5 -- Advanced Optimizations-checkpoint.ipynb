{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a14ed7",
   "metadata": {},
   "source": [
    "# Topic 5 -- Advanced Optimizations\n",
    "\n",
    "Welcome back! in the previous topic, you learned how Neural Networks are formed by **stacking** Logistic Regression layers on top of one another, creating a construct that can learn **much more complicated hypothesis functions**. While this allows you to tackle more challenging problems such as computer vision, having so many $w$ and $b$ parameters means Neural Networks **take much longer to train** and are **prone to overfitting**.\n",
    "\n",
    "In this notebook, we are going to cover various optimization techniques that can **dramatically speed up learning**, as well as advanced methods to **address overfitting**. Later on, we are going to revisit our **handwritten digits classifier** and see if we can **improve its performance** with our new knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ef227",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Big Data](#bigdata)\n",
    "    - [Stochastic Gradient Descent](#sgd)\n",
    "    - [Mini-Batch Gradient Descent](#minibatch)\n",
    "    \n",
    "    \n",
    "2. [Coding Exercise: Data Generator](#datagen)\n",
    "    - [Getting Familiar with `yield`](#yield)\n",
    "    - [Data Generator](#gen)\n",
    "    \n",
    "    \n",
    "3. [Dropout](#dropout)\n",
    "    - [A New Form of Regularization](#newreg)\n",
    "    - [Implementation in PyTorch](#dppy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe298434",
   "metadata": {},
   "source": [
    "### Before we Begin...\n",
    "\n",
    "Let's first import our modules as always. The modules used in this notebook the same as the ones used previously.\n",
    "\n",
    "- **Numpy**: Powerful linear algebra library\n",
    "- **Pandas**: Used for organizing our data\n",
    "- **SKLearn**: Abstract machine learning library\n",
    "- **MatPlotLib, Bokeh,** and **SeaBorn**: Data visualization libraries\n",
    "- **utils.py**: A custom python script that contains functions used in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8913f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf5a5e",
   "metadata": {},
   "source": [
    "## Big Data <a name=\"bigdata\"></a>\n",
    "\n",
    "Up until now, when training a learning algorithm, whether it is simple Linear/Logistic Regression or it's a Neural Network, we trained it by passing in the **entire dataset**. The act of training a ML algorithm by iterating over the entire training dataset at once is called **Batch Gradient Descent**.\n",
    "\n",
    "For now, batch gradient descent has worked for our purposes, however, there are two main issues when we mention batch gradient descent in the context of Neural Networks. **Firstly**, batch gradient descent is **slow** -- you need to make one pass over the **entire training set** before making **one gradient descent step**. The **second**, and more important issue is that often times in Deep Learning, we need to use datasets that are **massive** -- sometimes over 100,000 hi-res images can be used to train a Deep Neural Net. A computer simply **does not have enough memory** to load in that big of a dataset.\n",
    "\n",
    "### Stochastic Gradient Descent <a name=\"sgd\"></a>\n",
    "\n",
    "**Stochastic Gradient Descent** or **SGD** is probably something you have seen before. In our PyTorch portions of the course, we've used the `SGD()` optimizer to perform gradient descent on our cost function. <u>To make things clear, we used `SGD` to perform **Batch Gradient Descent** on all of our projects so far</u>, however, the formal concept of SGD is actually quite the opposite of batch gradient descent. Instead of feeding forward the entire training set and then updating the weights and biases, SGD will **feed in one training example** at a time, updating the parameters **after every training example**.\n",
    "\n",
    "##### Question for the Students:\n",
    "\n",
    "Other than much more frequent parameter updates, what are some other benefits of of SGD? What are some drawbacks?\n",
    "\n",
    "### Mini-Batch Gradient Descent <a name=\"minibatch\"></a>\n",
    "\n",
    "On one end of the extreme we have batch gradient descent which iterates on the entire training set, and on the other extreme we have Stochastic Gradient Descent that iterates on the individual training examples. **Mini-batch Gradient Descent** is the happy medium in between, where the training set is divided into multiple \"mini-batches\", and the learning algorithm iterates on those mini-batches.\n",
    "\n",
    "Often times mini-batch sizes range from 8 to 512. Generally, most people train with mini-batch sizes of 16, 32, 64, or 128, as these strike a good balance between update frequency and memory useage.\n",
    "\n",
    "##### Question for Students\n",
    "If SGD updates more frequently than mini-batch gradient descent, then why is mini-batch gradient descent the preferred method of training a Neural Network?\n",
    "    \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7925db4",
   "metadata": {},
   "source": [
    "## Coding Exercise: Data Generator <a name=\"datagen\"></a>\n",
    "\n",
    "A **data generator** is a program that helps \"funnel\" data into the Neural Network. Previously, we did not need to use a data generator because all we had to do was send the entire training data into the model. This data generator will **break up** the training set into mini-batches which are then fed into the model. In this section you will learn to use the `yield` statement in python, which creates a custom **iterable**.\n",
    "\n",
    "\n",
    "### Getting Familiar with `yield` <a name=\"yield\"></a>\n",
    "\n",
    "The `yield` statement creates an **iterable** that you can step through. You can almost treat it as the `return` statement, except it allows you to `return` a value from a function without actually terminating the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7ba181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arange():\n",
    "    n = 1\n",
    "    yield n\n",
    "    \n",
    "    n += 1 \n",
    "    yield n\n",
    "        \n",
    "    n += 1 \n",
    "    yield n\n",
    "        \n",
    "    n += 1 \n",
    "    yield n\n",
    "        \n",
    "    n += 1 \n",
    "    yield n\n",
    "        \n",
    "    n += 1 \n",
    "    yield n\n",
    "        \n",
    "    n += 1 \n",
    "    yield n\n",
    "        \n",
    "    n += 1 \n",
    "    yield n\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f94119",
   "metadata": {},
   "source": [
    "In the function above, we wrote 8 `yield` statements. By assigning this function to a variable, and **iterable** is created that allows you to step through using the keyword `next`. Try stepping through this iterable 8 times, or even more to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b558b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-644765955b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "numbers = arange()\n",
    "\n",
    "print(next(numbers))\n",
    "print(next(numbers))\n",
    "print(next(numbers))\n",
    "print(next(numbers))\n",
    "print(next(numbers))\n",
    "print(next(numbers))\n",
    "print(next(numbers))\n",
    "print(next(numbers))\n",
    "print(next(numbers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50f8d4",
   "metadata": {},
   "source": [
    "As you can see, once we call  our simple generator the 9<sup>th</sup> time, it raises a `StopIteration` exception. Now that we are familiar with python generators using the `yield` statement, we can continue to design our data generator. Lets take a look at our current training loop and see if we can make any modifications. Note that this is **pseudo-code.**\n",
    "\n",
    "```python\n",
    "def fit(X_train, Y_train)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        zero_grads()\n",
    "    \n",
    "        Y_pred = forward_prop(X_train)\n",
    "        cost = loss_fn(Y_train, Y_pred)\n",
    "    \n",
    "        grads = back_prop(cost)\n",
    "        update_weights(grads)\n",
    "\n",
    "```\n",
    "\n",
    "Here, notice that for every iteration, the **entire** training set is passed into the network via the `forward_prop()` function. We want to use **generators** to feed **mini-batches** into the model. \n",
    "\n",
    "Previously, we've always refered to **epochs** as **iterations**. The definition of one epoch is ***a full pass through the training set***, and in batch gradient descent, we pass the entire training set, or in other words, we pass the entire epoch every iteration.\n",
    "\n",
    "In mini-batch gradient descent, we iterate on the mini-batches, therefore we can iterate **multiple times** per epoch. Lets say that we have 100 training examples in our training data. If we choose a mini-batch size of 20, that means we can have 5 steps per epoch. With that said, here's the new `fit()` function:\n",
    "\n",
    "```python\n",
    "def train_generator(dataset):\n",
    "    ... some code here...\n",
    "    yield (X_train_batch, Y_train_batch)\n",
    "\n",
    "def fit_minibatch(train_generator):\n",
    "    for i in range(epochs):\n",
    "        for j in range(steps_per_epoch):\n",
    "            X_train_batch, Y_train_batch = next(train_generator)\n",
    "            \n",
    "            Y_pred_batch = feed_forward(X_train_batch)\n",
    "            cost = loss_fn(Y_train_batch, Y_pred_batch)\n",
    "            \n",
    "            grads = back_prop(cost)\n",
    "            update_weights(grads)\n",
    "\n",
    "```\n",
    "\n",
    "### Data Generator <a name=\"gen\"></a>\n",
    "\n",
    "Now it's time to put what we've gathered and create a data generator! First, let's create some simple **training data** we can use to test our generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d12fed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [11],\n",
       "       [12],\n",
       "       [13],\n",
       "       [14],\n",
       "       [15],\n",
       "       [16],\n",
       "       [17],\n",
       "       [18],\n",
       "       [19],\n",
       "       [20],\n",
       "       [21],\n",
       "       [22],\n",
       "       [23],\n",
       "       [24],\n",
       "       [25],\n",
       "       [26],\n",
       "       [27],\n",
       "       [28],\n",
       "       [29],\n",
       "       [30],\n",
       "       [31],\n",
       "       [32],\n",
       "       [33],\n",
       "       [34],\n",
       "       [35],\n",
       "       [36],\n",
       "       [37],\n",
       "       [38],\n",
       "       [39],\n",
       "       [40],\n",
       "       [41],\n",
       "       [42],\n",
       "       [43],\n",
       "       [44],\n",
       "       [45],\n",
       "       [46],\n",
       "       [47],\n",
       "       [48],\n",
       "       [49],\n",
       "       [50],\n",
       "       [51],\n",
       "       [52],\n",
       "       [53],\n",
       "       [54],\n",
       "       [55],\n",
       "       [56],\n",
       "       [57],\n",
       "       [58],\n",
       "       [59],\n",
       "       [60],\n",
       "       [61],\n",
       "       [62],\n",
       "       [63],\n",
       "       [64],\n",
       "       [65],\n",
       "       [66],\n",
       "       [67],\n",
       "       [68],\n",
       "       [69],\n",
       "       [70],\n",
       "       [71],\n",
       "       [72],\n",
       "       [73],\n",
       "       [74],\n",
       "       [75],\n",
       "       [76],\n",
       "       [77],\n",
       "       [78],\n",
       "       [79],\n",
       "       [80],\n",
       "       [81],\n",
       "       [82],\n",
       "       [83],\n",
       "       [84],\n",
       "       [85],\n",
       "       [86],\n",
       "       [87],\n",
       "       [88],\n",
       "       [89],\n",
       "       [90],\n",
       "       [91],\n",
       "       [92],\n",
       "       [93],\n",
       "       [94],\n",
       "       [95],\n",
       "       [96],\n",
       "       [97],\n",
       "       [98],\n",
       "       [99]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = np.arange(0, 100).reshape(-1, 1)\n",
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d073471",
   "metadata": {},
   "source": [
    "Here, we've created an array with shape of `(100, 1)`. This represents a training set with 100 examples and one feature per example. Next, let's build the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87abdf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(dataset, batch_size=16):\n",
    "    lower = 0\n",
    "    upper = batch_size\n",
    "    while(True):\n",
    "        \n",
    "        \n",
    "        if upper <= dataset.shape[0]:  # Normal operation\n",
    "            yield(dataset[lower:upper])\n",
    "        else:  # Wrap around\n",
    "            batch = dataset[lower:]\n",
    "            \n",
    "            # Calculate how many elements are already in the batch\n",
    "            already_added = dataset.shape[0]-lower\n",
    "            # How many still needs to be added\n",
    "            left_in_batch = batch_size - already_added            \n",
    "            # joins the last few training examples with the first few\n",
    "            batch = np.concatenate((batch, dataset[:left_in_batch]), axis=0)\n",
    "            \n",
    "            upper = left_in_batch\n",
    "            yield(batch)\n",
    "            \n",
    "        lower = upper\n",
    "        upper += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f2635fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]\n",
      " [13]\n",
      " [14]\n",
      " [15]] \n",
      "\n",
      "[[16]\n",
      " [17]\n",
      " [18]\n",
      " [19]\n",
      " [20]\n",
      " [21]\n",
      " [22]\n",
      " [23]\n",
      " [24]\n",
      " [25]\n",
      " [26]\n",
      " [27]\n",
      " [28]\n",
      " [29]\n",
      " [30]\n",
      " [31]] \n",
      "\n",
      "[[32]\n",
      " [33]\n",
      " [34]\n",
      " [35]\n",
      " [36]\n",
      " [37]\n",
      " [38]\n",
      " [39]\n",
      " [40]\n",
      " [41]\n",
      " [42]\n",
      " [43]\n",
      " [44]\n",
      " [45]\n",
      " [46]\n",
      " [47]] \n",
      "\n",
      "[[48]\n",
      " [49]\n",
      " [50]\n",
      " [51]\n",
      " [52]\n",
      " [53]\n",
      " [54]\n",
      " [55]\n",
      " [56]\n",
      " [57]\n",
      " [58]\n",
      " [59]\n",
      " [60]\n",
      " [61]\n",
      " [62]\n",
      " [63]] \n",
      "\n",
      "[[64]\n",
      " [65]\n",
      " [66]\n",
      " [67]\n",
      " [68]\n",
      " [69]\n",
      " [70]\n",
      " [71]\n",
      " [72]\n",
      " [73]\n",
      " [74]\n",
      " [75]\n",
      " [76]\n",
      " [77]\n",
      " [78]\n",
      " [79]] \n",
      "\n",
      "[[80]\n",
      " [81]\n",
      " [82]\n",
      " [83]\n",
      " [84]\n",
      " [85]\n",
      " [86]\n",
      " [87]\n",
      " [88]\n",
      " [89]\n",
      " [90]\n",
      " [91]\n",
      " [92]\n",
      " [93]\n",
      " [94]\n",
      " [95]] \n",
      "\n",
      "[[96]\n",
      " [97]\n",
      " [98]\n",
      " [99]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]] \n",
      "\n",
      "[[12]\n",
      " [13]\n",
      " [14]\n",
      " [15]\n",
      " [16]\n",
      " [17]\n",
      " [18]\n",
      " [19]\n",
      " [20]\n",
      " [21]\n",
      " [22]\n",
      " [23]\n",
      " [24]\n",
      " [25]\n",
      " [26]\n",
      " [27]] \n",
      "\n",
      "[[28]\n",
      " [29]\n",
      " [30]\n",
      " [31]\n",
      " [32]\n",
      " [33]\n",
      " [34]\n",
      " [35]\n",
      " [36]\n",
      " [37]\n",
      " [38]\n",
      " [39]\n",
      " [40]\n",
      " [41]\n",
      " [42]\n",
      " [43]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen = generator(trainset)\n",
    "\n",
    "print(next(gen), '\\n')\n",
    "print(next(gen), '\\n')\n",
    "print(next(gen), '\\n')\n",
    "print(next(gen), '\\n')\n",
    "print(next(gen), '\\n')\n",
    "print(next(gen), '\\n')\n",
    "print(next(gen), '\\n')\n",
    "print(next(gen), '\\n')\n",
    "print(next(gen), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8821abe",
   "metadata": {},
   "source": [
    "Awesome, looks like our generator works very well! We will write something similar for our programming project.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9735c",
   "metadata": {},
   "source": [
    "## Dropout <a name=\"dropout\"></a>\n",
    "\n",
    "In the previous topics, you learned about **overfitting** and **underfitting**, and how to address them. This is more important than ever when talking about Neural Networks. Since Neural Networks are able to **fit more complex relations**, it is **more prone to overfitting.** This is why over the years, computing scientists came up with more advanced methods of regulating the NN's weights and biases. \n",
    "\n",
    "### A New Form of Regularization <a name=\"newreg\"></a>\n",
    "\n",
    "So far, we've seen **L2 regularization** in action, smoothing out high degree polynomials so that they **generalize better**. While this works great for logistic regression, many times L2 regularization doesn't work as well for Neural Networks. That's not to say you *shouldn't* use L2, in fact more often you'll encounter NN's that use L2 than ones that do not. \n",
    "\n",
    "**Dropout** is a powerful method of regularization that often times work better than L2. It works by randomly turning off various neurons, which in turn results in a simpler network that is less prone to overfitting. In the figure below, notice how after every iteration, **a different set of neurons are turned off**.\n",
    "\n",
    "<img src=\"images/dropout.png\" alt=\"cannot display image\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1cc23",
   "metadata": {},
   "source": [
    "In addition to simplifying the network architecture, leading to less overfitting, dropout also **changes** the network architecture, so in short, your Neural Network could be fitting many different architectures to the problem.\n",
    "\n",
    "### Implementation in PyTorch <a name=\"dppy\"></a>\n",
    "\n",
    "In PyTorch, L2 reglularization was implemented in the optimizer, meaning that the entire Neural Network had the same L2 regularization parameter. Dropout gives you more flexibility, as **each layer** will have its own dropout module. Dropout is implemented **after** the layer's activation function:\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x = self.z1(x)\n",
    "    x = self.a1(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = self.z2(x)\n",
    "    x = self.a2(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = self.z3(x)\n",
    "    x = self.a3(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "  \n",
    "    x = self.z4(x)\n",
    "    x = self.a4(x)\n",
    "    \n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ade95",
   "metadata": {},
   "source": [
    "Here, we can pass a parameter into each dropout layer which represents the probability that a certain neuron is dropped. Notice that no dropout is applied to the output layer, as you do not want to randomly get rid of class predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e741a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
